{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe Teilmodul 2.1\n",
    "## Auswahl Ihrer Daten\n",
    "Suchen Sie auf bibsonomy mit einem selbst gewählten Suchbegriff nach Publikationen. Auf Bibsonomy lassen sich die Ergebnisse einer Keyword-Suche (z.B. \"Bibliothek\") in einer Vielzahl verschiedener Formate exportieren (Limit ggf. vorher auf 1000 erhöhen) und lokal speichern. Geben Sie dafür einen Suchbegriff oben rechts in die Maske ein und bestätigen Sie die Eingabe. Rechts neben der Überschrift der rechten Spalte (\"publications\"/\"Publikationen\") klicken Sie ganz rechte auf das Symbol mit dem Pfeil nach rechts, der aus einem Kasten zeigt (\"export options for displayed posts\"/\"Exportieren für angezeigte Einträge\"), dann auf mehr/more. Wählen Sie ZUERST bei posts/Einträge \"1000\" aus, und wählen sie dann in dem Format drop-Down-Menü fast ganz unten JSON aus. Es öffnet sich die JSON Datei in ihrem Browser.  Sie können die URL wie im Tutorial über das requests Modul in Python öffnen.\n",
    "\n",
    "Alternativ können Sie die Datei auch lokal speichern. Dafür rechtsklicken Sie irgendwo, wählen Sie \"save as...\"/\"speichern unter...\" und speichern Sie die Datei unter einem passenden Dateinamen mit der Endung \".json\".\n",
    "\n",
    "Für eine Nutzung im zweiten Modulteil ist ein großer Datensatz sinnvoll. Fügen Sie daher die Ergebnisse mehrerer Suchen mit Pandas in einem Datensatz zusammenführen.\n",
    "Fügen sie ihren Datensätzen dabei den Suchterm hinzu, der zu den Ergebnissen geführt hat. Tipp: Informationen dazu finden Sie unter \"Daten Verändern und Hinzufügen\".\n",
    "\n",
    "Nutzen sie dabei Python (vorzugsweise in einem Jupyter Notebook) mithilfe von Pandas. Sollten Sie Probleme haben, schauen Sie im Pandas Tutorial unter \"JSON einlesen\" oder bitten Sie Fabian Haak im Moodle Austauschforum um Hilfe.\n",
    "Legen Sie Ihr Notebook bzw. Python-Script mit den Lösungen in einem \"Modul_2-1\" Ordner im eigenen Repositorium auf GitHub ab. Fügen Sie mich als zusätzlichen Collaborator (GitHub-Name: HaakFab) hinzu, so kann ich besser Feedback oder Hilfestellungen geben.\n",
    "\n",
    "## Exportformat erstellen\n",
    "Diese Aufgabe dient als direkte Vorbereitung des nächsten Modulteils. Wir empfehlen, Pandas für die Bearbeitung der Aufgabe zu nutzen. \n",
    "\n",
    "### Datenaufbereitung\n",
    "Für den nächsten Teil dieses Modules möchten wir unseren Datensatz wieder als JSON-File ablegen. Zunächst möchten wir aber die Bibsonomy-Daten noch etwas zu bereinigen:\n",
    "- Filtern Sie die Daten so, dass nur noch Einträge des Typs \"Publication\" enthalten sind.\n",
    "- Überlegen Sie, welche Felder Sie möglicherweise umbenennen wollen, und welche Sie nicht benötigen. Benennen Sie entsprechend um und wählen Sie nur die Spalten, die für Sie sinnvolle Informationen enthalten. Mögliche Änderungen der Daten sind das Umbenennen von Felder zu eindeutigeren Bezeichnungen (wie beispielsweise im Pandas Tutorial \"label\" zu \"title\"). Filtern Sie ihre Daten jedoch nicht zu drastisch und entfernen Sie nicht zu viele Felder, damit im zweiten Modulteil noch genügend Informationen übrig bleiben.\n",
    "- Wenn sie mehrere Suchergebnisse zusammengeführt haben, ist es sinnvoll, Nachduplikaten zu überprüfen.\n",
    "- BONUS: Wie bei den Jareszahlen ist es durchaus sinnvoll, zu überprüfen, ob alle Einträge \"sauber\" vorliegen. Stellen sie dafür sicher, dass alle Einträge Informationen zu Titel, AutorInnen und Veröffentlichungsjahr enthalten und dass das Jahr eine sinnvolle Zahl ist!\n",
    "Daten Export\n",
    "\n",
    "### Erstellen Sie einen sauberen Export im JSON-Format, das von Solr akzeptiert wird.\n",
    "\n",
    "Für die Weiterverwendung ist nicht relevant, wie die einzelnen Felder benannt sind oder wie viele Felder es gibt, Es ist aber entscheidend, dass die äußere Form einer Liste von Dictionaries gewahrt wird. (HINWEIS BEI PROBLEMEN: Schauen Sie im Pandas Tutorial unter \"Export als JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 Ergebnisse für query artificial intelligence gefunden.\n",
      "2000 Ergebnisse für query AI gefunden.\n",
      "2000 Ergebnisse für query machine learning gefunden.\n",
      "1311 Ergebnisse für query language model gefunden.\n",
      "1410 Ergebnisse für query KI gefunden.\n",
      "1275 Ergebnisse für query künstliche intelligenz gefunden.\n",
      "295 Ergebnisse für query maschinelles lernen gefunden.\n",
      "30 Ergebnisse für query sprachmodell gefunden.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# zum Zusammenbauen der Bibsonomy URL brauchen wir alles, was vor und nach der Query kommt\n",
    "url_base = \"https://www.bibsonomy.org/json/search/\"\n",
    "url_attr = \"?items=1000\"\n",
    "\n",
    "def df_from_query(query):\n",
    "    # die URL; wir müssen Leerzeichen ersetzen mit \"%20\" nach ASCII Encoding guideline\n",
    "    url = url_base + query.replace(\" \", \"%20\") + url_attr\n",
    "    \n",
    "    result = requests.get(url)\n",
    "    json_data = result.json()\n",
    "    \n",
    "    # im Feld \"items\" stecken die relevanten Dateneinträge\n",
    "    dataframe = pd.DataFrame(json_data[\"items\"])\n",
    "\n",
    "    # wir filtern so, dass nur Publikationen bleiben\n",
    "    dataframe.loc[dataframe[\"type\"] == \"Publication\"]\n",
    "\n",
    "    # wir möchten die query in die Daten eintragen \n",
    "    dataframe[\"query\"] = query\n",
    "    \n",
    "    # die Rückgabe der Methode nicht vergessen:\n",
    "    return dataframe\n",
    "\n",
    "# wir erstellen eine leere Liste, in die wir unsere Dataframes packen\n",
    "dataframes = []\n",
    "\n",
    "# eine Liste von Suchanfragen\n",
    "queries = [\"artificial intelligence\", \"AI\", \"machine learning\", \"language model\", \"KI\", \"künstliche intelligenz\", \"maschinelles lernen\", \"sprachmodell\"]\n",
    "\n",
    "# jetzt gehen wir die Liste von Suchanfragen durch und rufen unsere Funktion für jede Suchanfrage auf\n",
    "for query in queries:\n",
    "    df = df_from_query(query)\n",
    "    # schließlich fügen wir das DataFrame über append unserer Liste von Dataframes hinzu\n",
    "    dataframes.append(df)\n",
    "\n",
    "    # wie wir uns eine Statusausgabe schreiben würden:\n",
    "    # print(\"Ergebnisse für \", query , \" gefunden. Dataframe enthält \",len(df), \" Einträge.\")\n",
    "    \n",
    "    # mit einem f String lassen sich Variablen in Strings viel einfacher formatieren:\n",
    "    print(f\"{len(df)} Ergebnisse für query {query} gefunden.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Da die dataframes die selbe From (column titles sind identisch, wenn für alle vorhanden) haben, können wir die Dataframes über concat zusammenfügen\n",
    "data_conc = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtern Sie die Daten so, dass nur noch Einträge des Typs \"Publication\" enthalten sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_conc.loc[data_conc[\"type\"] == \"Publication\"] #war vorher noch nicht gespeichter, nur als Ansicht (kein \" = \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Überlegen Sie, welche Felder Sie möglicherweise umbenennen wollen, und welche Sie nicht benötigen. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['type',\n",
       " 'id',\n",
       " 'tags',\n",
       " 'intraHash',\n",
       " 'label',\n",
       " 'user',\n",
       " 'description',\n",
       " 'date',\n",
       " 'changeDate',\n",
       " 'count',\n",
       " 'url',\n",
       " 'interHash',\n",
       " 'pub-type',\n",
       " 'journal',\n",
       " 'year',\n",
       " 'author',\n",
       " 'authors',\n",
       " 'editor',\n",
       " 'editors',\n",
       " 'volume',\n",
       " 'number',\n",
       " 'pages',\n",
       " 'abstract',\n",
       " 'ee',\n",
       " 'language',\n",
       " 'issn',\n",
       " 'bibtexKey',\n",
       " 'publisher',\n",
       " 'address',\n",
       " 'isbn',\n",
       " 'file',\n",
       " 'urldate',\n",
       " 'series',\n",
       " 'refid',\n",
       " 'note',\n",
       " 'doi',\n",
       " 'shorttitle',\n",
       " 'booktitle',\n",
       " 'location',\n",
       " 'ean',\n",
       " 'asin',\n",
       " 'dewey',\n",
       " 'numpages',\n",
       " 'issue_date',\n",
       " 'acmid',\n",
       " 'owner',\n",
       " 'pmid',\n",
       " 'timestamp',\n",
       " 'username',\n",
       " 'intrahash',\n",
       " 'added-at',\n",
       " 'numer',\n",
       " 'interhash',\n",
       " 'biburl',\n",
       " 'groups',\n",
       " 'notes',\n",
       " 'size',\n",
       " 'citeulike-article-id',\n",
       " 'priority',\n",
       " 'comment',\n",
       " 'book',\n",
       " 'date-modified',\n",
       " '_originaltype',\n",
       " '_keywords',\n",
       " '_originalcomment',\n",
       " 'eventtitle',\n",
       " 'eventdate',\n",
       " 'googleid',\n",
       " 'cluster',\n",
       " 'citeseer-isreferencedby',\n",
       " 'rights',\n",
       " 'oai',\n",
       " 'serial',\n",
       " 'articleno',\n",
       " 'pagetotal',\n",
       " 'ppn_gvk',\n",
       " 'subtitle',\n",
       " 'local-url',\n",
       " 'oldnote',\n",
       " 'posted-at',\n",
       " 'date-added',\n",
       " 'uri',\n",
       " 'pii',\n",
       " 'affiliation',\n",
       " 'rating',\n",
       " 'eprint',\n",
       " 'lastchecked',\n",
       " 'source',\n",
       " 'archiveprefix',\n",
       " 'primaryclass',\n",
       " 'citeulike-linkout-9',\n",
       " 'citeulike-linkout-5',\n",
       " 'citeulike-linkout-6',\n",
       " 'citeulike-linkout-7',\n",
       " 'citeulike-linkout-8',\n",
       " 'citeulike-linkout-1',\n",
       " 'citeulike-linkout-2',\n",
       " 'citeulike-linkout-3',\n",
       " 'citeulike-linkout-4',\n",
       " 'citeulike-linkout-0',\n",
       " 'citeulike-linkout-10',\n",
       " 'citeulike-linkout-11',\n",
       " 'publisher_address',\n",
       " 'venue',\n",
       " 'imtm-standort',\n",
       " 'imtm-signatur',\n",
       " 'imtm-fullref',\n",
       " 'imtm-code',\n",
       " 'imtm-inventarnummer',\n",
       " 'copyright',\n",
       " 'bdsk-url-1',\n",
       " 'annotation',\n",
       " 'subject',\n",
       " 'bibsource',\n",
       " 'identifier',\n",
       " 'format',\n",
       " 'review',\n",
       " 'email',\n",
       " 'topic',\n",
       " 'xref',\n",
       " 'supervisor',\n",
       " 'journaltitle',\n",
       " 'doc-delivery-number',\n",
       " 'number-of-cited-references',\n",
       " 'unique-id',\n",
       " 'paperid',\n",
       " 'text',\n",
       " 'folder',\n",
       " 'conference',\n",
       " 'imtm-stichworte',\n",
       " 'imtm-auswahlen',\n",
       " 'keyword',\n",
       " 'issue',\n",
       " 'nocites',\n",
       " 'bdsk-file-1',\n",
       " 'optvolume',\n",
       " 'position',\n",
       " 'category',\n",
       " 'impact',\n",
       " 'quartile',\n",
       " 'dnbtitleid',\n",
       " 'pdf',\n",
       " 'broken',\n",
       " 'organisation',\n",
       " 'citedreferences',\n",
       " 'optseries',\n",
       " 'query',\n",
       " 'shop',\n",
       " 'sortdate',\n",
       " 'isbn10',\n",
       " 'article-number',\n",
       " 'url_ppt',\n",
       " 'url_slides',\n",
       " 'page',\n",
       " 'audit-trail',\n",
       " 'lccn',\n",
       " 'addendum',\n",
       " 'pubmedid',\n",
       " 'abstractnote',\n",
       " 'place',\n",
       " 'collection',\n",
       " 'adsurl',\n",
       " 'adsnote',\n",
       " 'mrclass',\n",
       " 'mrnumber',\n",
       " 'mrnumber-url',\n",
       " 'bibdate',\n",
       " 'acknowledgement',\n",
       " 'reviewer',\n",
       " 'coden',\n",
       " 'mendeley-tags',\n",
       " 'arxivid',\n",
       " 'citeseerurl',\n",
       " 'library',\n",
       " 'file-x',\n",
       " 'codeurl',\n",
       " 'pmcid',\n",
       " 'fjournal',\n",
       " 'hdsurl',\n",
       " 'uniqueid',\n",
       " 'xcrossref',\n",
       " 'titleaddon',\n",
       " 'booktitleadon',\n",
       " 'funding-acknowledgement',\n",
       " 'journal-iso',\n",
       " 'author-email',\n",
       " 'keywords-plus',\n",
       " 'subject-category',\n",
       " 'funding-text',\n",
       " 'times-cited',\n",
       " 'hal_id',\n",
       " 'audience',\n",
       " 'elocation-id',\n",
       " 'optcrossref',\n",
       " 'presentation_start',\n",
       " 'session',\n",
       " 'track',\n",
       " 'presentation_end',\n",
       " 'room',\n",
       " 'eid',\n",
       " 'cdrom',\n",
       " 'revision',\n",
       " 'price',\n",
       " 'translator',\n",
       " 'keywords',\n",
       " 'intext',\n",
       " 'collaborator',\n",
       " 'vgwort',\n",
       " 'entrytype',\n",
       " 'author+an',\n",
       " 'yearoptique',\n",
       " 'partneroptique',\n",
       " 'openaccess',\n",
       " 'wpoptique',\n",
       " 'bdsk-url-2',\n",
       " 'read',\n",
       " 'md5sum',\n",
       " 'citations',\n",
       " 'pdfmeat',\n",
       " 'citedbyid',\n",
       " 'mailhosts']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sel = data[[\n",
    "    \"id\",\n",
    "    \"label\",\n",
    "    \"tags\",\n",
    "    \"url\",\n",
    "    \"authors\",\n",
    "    \"year\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6219 entries, 1000 to 10320\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       6219 non-null   object\n",
      " 1   label    6219 non-null   object\n",
      " 2   tags     6219 non-null   object\n",
      " 3   url      6219 non-null   object\n",
      " 4   authors  5928 non-null   object\n",
      " 5   year     6219 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 340.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data_sel.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benennen Sie entsprechend um und wählen Sie nur die Spalten, die für Sie sinnvolle Informationen enthalten. Mögliche Änderungen der Daten sind das Umbenennen von Felder zu eindeutigeren Bezeichnungen (wie beispielsweise im Pandas Tutorial \"label\" zu \"title\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sel = data_sel.rename(columns={\"label\":\"title\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wenn sie mehrere Suchergebnisse zusammengeführt haben, ist es sinnvoll, nach Duplikaten zu überprüfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sel_dup = data_sel.drop_duplicates(subset=\"title\", keep=\"last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SABINE:\n",
    "- Sollte man Dublikate vor dem Merge entfernen lassen oder lieber danach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BONUS: Wie bei den Jareszahlen ist es durchaus sinnvoll, zu überprüfen, ob alle Einträge \"sauber\" vorliegen. Stellen sie dafür sicher, dass alle Einträge Informationen zu Titel, AutorInnen und Veröffentlichungsjahr enthalten und dass das Jahr eine sinnvolle Zahl ist!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_sel_dup.dropna(subset=[\"title\", \"authors\", \"year\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wenn noch Zeit ist (sonst: wird nachgereicht!)\n",
    "\n",
    "#### LARA:\n",
    "- habe bei der Autoren-Spalte alle NaN Einträge rausgelöscht später aber festgestellt, dass ich noch solche Einträge habe: [?]\n",
    "Wie könnte ich diese rausfiltern und löschen sodass ich am Ende nur Namen in der Spalte habe?\n",
    "- in der Spalte \"language\" hatte ich Titel auf deutsch und englisch. Deutsche Titel hatten bspws. folgende Werte: dt, ger, german\n",
    "Wie kann ich diese Werte alle in \"deutsch\" umbenennen?\n",
    "\n",
    "\n",
    "\n",
    "1. Language Spalte säubern --> nur noch EN oder DE als Werte! Bei keiner angabe EN eintragen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gruppenaufgabe 1\n",
    "### Aufgabenbeschreibung\n",
    "Wir möchten nun zusätzlich zu den Bibsonomy Daten auch noch Daten aus einer weiteren Quelle zusammenführen, in diesem Fall Semantic Scholar.\n",
    "\n",
    "1. Lade \"bibsonomy.json\" und \"semantic_scholar.json\" in Pandas als ein Dataframe, füge eine Spalte \"source\" hinzu, in der du festhälst, woher die Suchergebnisse kommen  (also: bibsonomy oder semantic_scholar).\n",
    "2. Selektiere und benenne die Spalten so um, dass aus beiden Quellen Titel, Autoren, URL, Abstract, Keywords, Query (Suchanfrage) sowie das Jahr enthalten und gleich benannt sind.\n",
    "3. Stelle sicher, dass die Autoren gleich als Liste formatiert sind (also z. B. [Max Mustermann, Lieschen Müller] oder [Mustermann M., Müller L.]).\n",
    "4. Führe die Datensätze zusammen.\n",
    "5. Stelle sicher, dass alle Einträge für \"Jahr\" eine 4-stellige Zahl (int) sind.\n",
    "6. Entferne Duplikate (anhand der Spalte Title).\n",
    "7. Sorge dafür, dass es einen durchgängigen Index gibt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
